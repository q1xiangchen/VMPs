{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf12be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "# TODO:\n",
    "# 1. input length & norm mapping order\n",
    "\n",
    "def rearrange_tensor(input_tensor, order):\n",
    "    order = order.upper()\n",
    "    assert len(set(order)) == 5, \"Order must be a 5 unique character string\"\n",
    "    assert all([dim in order for dim in \"BCHWT\"]), \"Order must contain all of BCHWT\"\n",
    "    assert all([dim in \"BCHWT\" for dim in order]), \"Order must not contain any characters other than BCHWT\"\n",
    "\n",
    "    return input_tensor.permute([order.index(dim) for dim in \"BTCHW\"])\n",
    "\n",
    "def reverse_rearrange_tensor(input_tensor, order):\n",
    "    order = order.upper()\n",
    "    assert len(set(order)) == 5, \"Order must be a 5 unique character string\"\n",
    "    assert all([dim in order for dim in \"BCHWT\"]), \"Order must contain all of BCHWT\"\n",
    "    assert all([dim in \"BCHWT\" for dim in order]), \"Order must not contain any characters other than BCHWT\"\n",
    "\n",
    "    return input_tensor.permute([\"BTCHW\".index(dim) for dim in order])\n",
    "\n",
    "class MotionPrompt(torch.nn.Module):\n",
    "    def __init__(self, exp_name = \"\"):\n",
    "        super(MotionPrompt, self).__init__()\n",
    "        # default configs\n",
    "        self.input_permutation = \"BCTHW\"   # Input format from demo video reader\n",
    "        self.input_color_order = \"BGR\"     # Input format from demo video reader\n",
    "        self.gray_scale = {\"B\": 0.114, \"G\": 0.587, \"R\": 0.299}\n",
    "        self.visual = False\n",
    "        self.attention_map = None\n",
    "\n",
    "        # experimental configs\n",
    "        if exp_name.startswith(\"exp1\"):\n",
    "            self.attention_map = exp1\n",
    "        elif exp_name.startswith(\"exp2\"):\n",
    "            self.attention_map = exp2\n",
    "\n",
    "        if \"_1\" in exp_name:\n",
    "            self.a = nn.Parameter(torch.zeros(1)+1e-1)\n",
    "            self.b = nn.Parameter(torch.zeros(1))\n",
    "        else:\n",
    "            self.a = nn.Parameter(torch.nn.init.normal_(torch.empty(224,224), 0, 1))\n",
    "            self.b = nn.Parameter(torch.nn.init.normal_(torch.empty(224,224), 0, 1))\n",
    "\n",
    "        self.lambda1 = 0\n",
    "        if \"_loss\" in exp_name:\n",
    "            # self.c1 = 1e-3\n",
    "            # self.c2 = 1e-2\n",
    "            self.lambda1 = 1\n",
    "        if \"_loss_0.1\" in exp_name:\n",
    "            self.lambda1 = 0.1\n",
    "        elif \"_loss_0.5\" in exp_name:\n",
    "            self.lambda1 = 0.5\n",
    "        elif \"_loss_2\" in exp_name:\n",
    "            self.lambda1 = 2    \n",
    "        \n",
    "    def forward(self, video_seq):\n",
    "        print(f\"video_seq shape: {video_seq.shape}\")\n",
    "        print(f\"video_seq min: {video_seq.min()}, max: {video_seq.max()}\\n\")\n",
    "        video_seq = rearrange_tensor(video_seq, self.input_permutation)\n",
    "        loss = 0\n",
    "        \n",
    "        # normalize the input tensor back to [0, 1]\n",
    "        norm_seq = video_seq * 0.225 + 0.45\n",
    "        \n",
    "        frame_check_acc = 0\n",
    "        for i in range(norm_seq.shape[0]):\n",
    "            temp_seq = norm_seq[i]\n",
    "            frame_check = [(temp_seq[i] == temp_seq[i+1]).all() for i in range(temp_seq.shape[0]-1)]\n",
    "            frame_check_acc += 0 if sum(frame_check) == 0 else sum(frame_check) + 1\n",
    "\n",
    "        # transfor the input tensor to grayscale \n",
    "        weights = torch.tensor([self.gray_scale[idx] for idx in self.input_color_order], \n",
    "                               dtype=norm_seq.dtype, device=norm_seq.device)\n",
    "        grayscale_video_seq = torch.einsum(\"btcwh, c -> btwh\", norm_seq, weights)\n",
    "        print(f\"grayscale_video_seq shape: {grayscale_video_seq.shape}\")\n",
    "        print(f\"grayscale_video_seq min: {grayscale_video_seq.min()}, max: {grayscale_video_seq.max()}\\n\")\n",
    "        \n",
    "        ### frame difference & sums & counts & ratios ###\n",
    "        B, T, H, W = grayscale_video_seq.shape\n",
    "        frame_diff = grayscale_video_seq[:,1:] - grayscale_video_seq[:,:-1]\n",
    "        print(f\"frame_diff shape: {frame_diff.shape}\")\n",
    "        print(f\"frame diff min: {frame_diff.min()}, max: {frame_diff.max()}\\n\")\n",
    "\n",
    "        ### power normalization ###\n",
    "        norm_attention = self.attention_map(frame_diff, self.a, self.b).unsqueeze(2)\n",
    "        print(f\"norm_attention shape: {norm_attention.shape}\")\n",
    "        print(f\"norm attention min: {norm_attention.min()}, max: {norm_attention.max()}\\n\")\n",
    "        pad_norm_attention = norm_attention.repeat(1, 1, 3, 1, 1)\n",
    "        print(f\"pad_norm_attention shape: {pad_norm_attention.shape}\")\n",
    "        print(f\"pad norm attention min: {pad_norm_attention.min()}, max: {pad_norm_attention.max()}\\n\")\n",
    "\n",
    "        if torch.is_grad_enabled():\n",
    "            # variance_loss_a = torch.var(self.a) / (H*W)\n",
    "            # variance_loss_b = torch.var(self.b) / (H*W)\n",
    "            temp_diff = norm_attention[:, 1:] - norm_attention[:, :-1]\n",
    "            temporal_loss = torch.sum(temp_diff.pow(2)) / (H*W*(T-2)*B)\n",
    "            # loss = self.lambda1 * temporal_loss + self.c1 * variance_loss_a + self.c2 * variance_loss_b \n",
    "            # print(f\"variance_loss_a: {variance_loss_a}\\t variance_loss_b: {variance_loss_b}\\t temporal_loss: {temporal_loss}\")\n",
    "            # if self.visual:\n",
    "            #     wandb.log({\n",
    "            #         \"variance_loss_a\": variance_loss_a, \"variance_loss_b\": variance_loss_b, \"temporal_loss\": temporal_loss\n",
    "            #     })\n",
    "            loss = self.lambda1 * temporal_loss\n",
    "            if self.visual:\n",
    "                wandb.log({\n",
    "                    \"temporal_loss\": loss\n",
    "                })\n",
    "\n",
    "        if self.visual:\n",
    "            wandb.log({\n",
    "            \"self.a.mean\": self.a.data[0].mean(), \"self.a.std\": self.a.data[0].std(), \n",
    "            \"self.b.mean\": self.b.data[0].mean(), \"self.b.std\": self.b.data[0].std(),\n",
    "            \"frame_check\": frame_check_acc\n",
    "            })\n",
    "\n",
    "        return reverse_rearrange_tensor((pad_norm_attention * video_seq[:,1:]), self.input_permutation), loss\n",
    "\n",
    "\n",
    "def exp1(input, a, b):\n",
    "    return 1 / (1 + torch.exp(\n",
    "        -(5 / (0.45 * torch.abs(torch.tanh(a))+1e-1)) * (input - 0.6 * torch.tanh(b))\n",
    "        ))\n",
    "\n",
    "def exp2(input, a, b):\n",
    "    return 1 / (1 + torch.exp( - torch.nn.ReLU()(a + 1e-3) * (input - b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67903f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_seq shape: torch.Size([8, 3, 9, 224, 224])\n",
      "video_seq min: -2.0, max: 2.4444446563720703\n",
      "\n",
      "grayscale_video_seq shape: torch.Size([8, 9, 224, 224])\n",
      "grayscale_video_seq min: 0.15650144219398499, max: 0.8219897747039795\n",
      "\n",
      "frame_diff shape: torch.Size([8, 8, 224, 224])\n",
      "frame diff min: -0.4549407958984375, max: 0.45778119564056396\n",
      "\n",
      "norm_attention shape: torch.Size([8, 8, 1, 224, 224])\n",
      "norm attention min: 1.2509172847785521e-05, max: 0.9997380375862122\n",
      "\n",
      "pad_norm_attention shape: torch.Size([8, 8, 3, 224, 224])\n",
      "pad norm attention min: 1.2509172847785521e-05, max: 0.9997380375862122\n",
      "\n",
      "output shape: torch.Size([8, 3, 8, 224, 224])\n",
      "output min: -1.615512490272522, max: 2.0868873596191406\n"
     ]
    }
   ],
   "source": [
    "model = MotionPrompt(exp_name=\"exp2_loss_0.5\")\n",
    "# print(model.attention_map.__name__)\n",
    "# print(model.lambda1)\n",
    "\n",
    "# B, C, T, H, W\n",
    "input = torch.randn(8, 3, 9, 224, 224)\n",
    "input = (input - input.min()) / (input.max() - input.min())\n",
    "input = (input - 0.45) / 0.225\n",
    "output, loss = model(input)\n",
    "print(f\"output shape: {output.shape}\")\n",
    "print(f\"output min: {output.min()}, max: {output.max()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
